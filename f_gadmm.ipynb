{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))/255\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))/255\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4  # first convolutional layer output depth\n",
    "L = 8  # second convolutional layer output depth\n",
    "M = 12  # third convolutional layer\n",
    "N = 200  # fully connected layer\n",
    "stride1 = 1  # output is 28x28\n",
    "stride2 = 2  # output is 7x7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    x_in = tf.placeholder(tf.float32,shape=[None, 784])\n",
    "    y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "    x_image = tf.reshape(x_in,[-1,28,28,1])\n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))\n",
    "    lambda_W1 = tf.zeros([5, 5, 1, K])\n",
    "    b1 = tf.Variable(tf.ones([K])/10)\n",
    "    lambda_b1 = tf.zeros([K])\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    lambda_W2 = tf.zeros([5, 5, K, L])\n",
    "    b2 = tf.Variable(tf.ones([L])/10)\n",
    "    lambda_b2 = tf.zeros([L])\n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    lambda_W3 = tf.zeros([4, 4, L, M])\n",
    "    b3 = tf.Variable(tf.ones([M])/10)\n",
    "    lambda_b3 = tf.zeros([M])\n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "    lambda_W4 = tf.zeros([7 * 7 * M, N])\n",
    "    b4 = tf.Variable(tf.ones([N])/10)\n",
    "    lambda_b4 = tf.zeros([N])\n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    lambda_W5 = tf.zeros([N, 10])\n",
    "    b5 = tf.Variable(tf.ones([10])/10)\n",
    "    lambda_b5 = tf.zeros([10])\n",
    "    #The model\n",
    "    y1 = tf.nn.relu(tf.nn.conv2d(x_image, W1, strides=[1, stride1, stride1, 1], padding='SAME') + b1)\n",
    "    y2 = tf.nn.relu(tf.nn.conv2d(y1, W2, strides=[1, stride2, stride2, 1], padding='SAME') + b2)\n",
    "    y3 = tf.nn.relu(tf.nn.conv2d(y2, W3, strides=[1, stride2, stride2, 1], padding='SAME') + b3)\n",
    "    # reshaping the output from the third convolution for the fully connected layer\n",
    "    yy = tf.reshape(y3, shape=[-1, 7 * 7 * M])\n",
    "    y4 = tf.nn.relu(tf.matmul(yy, W4) + b4)\n",
    "    ylogits = tf.matmul(y4, W5) + b5\n",
    "    y_pred = tf.nn.softmax(ylogits)\n",
    "    x = x_train\n",
    "    y = y_train\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [] # Creating the list of 10 users\n",
    "x_data = []\n",
    "y_data = []\n",
    "entropies = [] # Cross entropies of all users\n",
    "numberOfUsers = 10\n",
    "# Create 100 user objects \n",
    "for nums in range(numberOfUsers):\n",
    "    # Create a user\n",
    "    user = User()\n",
    "    entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits= user.ylogits, labels= user.y_true))*100\n",
    "    xData = user.x[nums:nums+1000]\n",
    "    yData = user.y[nums:nums+1000]\n",
    "    users.append(user)\n",
    "    entropies.append(entropy)\n",
    "    x_data.append(xData)\n",
    "    y_data.append(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = tf.constant(1e1)\n",
    "\n",
    "def sum_lambdas_w(user_a, user_b):\n",
    "    return tf.reduce_sum(tf.compat.v1.linalg.diag_part(tf.math.multiply(user_a.lambda_W1,(user_a.W1-user_b.W1))))+  tf.reduce_sum(tf.compat.v1.linalg.diag_part(tf.math.multiply(user_a.lambda_W2,(user_a.W2-user_b.W2)))) + tf.reduce_sum(tf.compat.v1.linalg.diag_part(tf.math.multiply(user_a.lambda_W3,(user_a.W3-user_b.W3)))) + tf.reduce_sum(tf.compat.v1.linalg.diag_part(tf.math.multiply(user_a.lambda_W4,(user_a.W4-user_b.W4)))) + tf.reduce_sum(tf.compat.v1.linalg.diag_part(tf.math.multiply(user_a.lambda_W5,(user_a.W5-user_b.W5))))\n",
    "\n",
    "\n",
    "def sum_lambdas_b(user_a, user_b):\n",
    "    return tf.tensordot(tf.transpose(user_a.lambda_b1), (user_a.b1- user_b.b1),1)+tf.tensordot(tf.transpose(user_a.lambda_b2), (user_a.b2- user_b.b2),1)+tf.tensordot(tf.transpose(user_a.lambda_b3), (user_a.b3- user_b.b3),1)+tf.tensordot(tf.transpose(user_a.lambda_b4), (user_a.b4- user_b.b4),1)+tf.tensordot(tf.transpose(user_a.lambda_b5), (user_a.b5- user_b.b5),1)\n",
    "\n",
    "def lambdas_w_update(user_a, user_b):    \n",
    "    first_layer= user_a.lambda_W1 + rho*(user_a.W1-user_b.W1)\n",
    "    second_layer= user_a.lambda_W2 + rho*(user_a.W2-user_b.W2)\n",
    "    third_layer= user_a.lambda_W3 + rho*(user_a.W3-user_b.W3)\n",
    "    fourth_layer= user_a.lambda_W4 + rho*(user_a.W4-user_b.W4)\n",
    "    fifth_layer= user_a.lambda_W5 + rho*(user_a.W5-user_b.W5)\n",
    "    \n",
    "    return first_layer,second_layer,third_layer,fourth_layer,fifth_layer\n",
    "\n",
    "def lambdas_b_update(user_a, user_b):    \n",
    "    first_layer= user_a.lambda_b1 + rho*(user_a.b1-user_b.b1)\n",
    "    second_layer= user_a.lambda_b2 + rho*(user_a.b2-user_b.b2)\n",
    "    third_layer= user_a.lambda_b3 + rho*(user_a.b3-user_b.b3)\n",
    "    fourth_layer= user_a.lambda_b4 + rho*(user_a.b4-user_b.b4)\n",
    "    fifth_layer= user_a.lambda_b5 + rho*(user_a.b5-user_b.b5)\n",
    "    \n",
    "    return first_layer,second_layer,third_layer,fourth_layer,fifth_layer\n",
    "    \n",
    "regW =0\n",
    "regb =0\n",
    "lambda_w = 0\n",
    "lambda_b = 0\n",
    "\n",
    "for i in range(numberOfUsers):\n",
    "    if i== 9:\n",
    "        break\n",
    "    user_a = users[i]\n",
    "    user_b = users[i+1]    \n",
    "    regW += rho/2*tf.nn.l2_loss(user_a.W1-user_b.W1)+ rho/2*tf.nn.l2_loss(user_a.W2-user_b.W2)+ rho/2*tf.nn.l2_loss(user_a.W3-user_b.W3)+ rho/2*tf.nn.l2_loss(user_a.W4-user_b.W4)+ rho/2*tf.nn.l2_loss(user_a.W5-user_b.W5)\n",
    "    regb += rho/2*tf.nn.l2_loss(user_a.b1-user_b.b1)+ rho/2*tf.nn.l2_loss(user_a.b2-user_b.b2)+ rho/2*tf.nn.l2_loss(user_a.b3-user_b.b3)+ rho/2*tf.nn.l2_loss(user_a.b4-user_b.b4)+ rho/2*tf.nn.l2_loss(user_a.b5-user_b.b5)\n",
    "    lambda_w += sum_lambdas_w(user_a, user_b)\n",
    "    lambda_b += sum_lambdas_b(user_a, user_b)\n",
    "    \n",
    "reg = regW+regb\n",
    "lambdas = lambda_w + lambda_b\n",
    "loss = cross_entropy + reg + lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for i in range(len(users)):\n",
    "    # Create a user\n",
    "    user = users[i]\n",
    "    \n",
    "    one_training = optimizer.minimize(loss, var_list=[user.W1, user.W2, user.W3, user.W4, user.W5, user.b1, user.b2, user.b3, user.b4, user.b5])\n",
    "    \n",
    "    train_data.append(one_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, mb_size = 100):\n",
    "\n",
    "    m = X.shape[0]\n",
    "\n",
    "    perm = list(np.random.permutation(m))\n",
    "    #perm = perm_init[0:100]\n",
    "    X_temp = X[perm,:]\n",
    "    Y_temp = Y[perm,:].reshape((m, Y.shape[1]))\n",
    "    \n",
    "    X_r = X_temp[0:mb_size,:]\n",
    "    Y_r = Y_temp[0:mb_size,:]\n",
    "    return X_r,Y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avgAcc = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(100):\n",
    "\n",
    "        batch_x1 , batch_y1 = mini_batches(x_data[0],y_data[0], 100)\n",
    "        batch_x2 , batch_y2 = mini_batches(x_data[1],y_data[1], 100)\n",
    "        batch_x3 , batch_y3 = mini_batches(x_data[2],y_data[2], 100)\n",
    "        batch_x4 , batch_y4 = mini_batches(x_data[3],y_data[3], 100)\n",
    "        batch_x5 , batch_y5 = mini_batches(x_data[4],y_data[4], 100)\n",
    "        batch_x6 , batch_y6 = mini_batches(x_data[5],y_data[5], 100)\n",
    "        batch_x7 , batch_y7 = mini_batches(x_data[6],y_data[6], 100)\n",
    "        batch_x8 , batch_y8 = mini_batches(x_data[7],y_data[7], 100)\n",
    "        batch_x9 , batch_y9 = mini_batches(x_data[8],y_data[8], 100)\n",
    "        batch_x10 , batch_y10 = mini_batches(x_data[9],y_data[9], 100)\n",
    "        \n",
    "        sess.run(train_data[0],feed_dict={users[0].x_in:batch_x1,users[0].y_true:batch_y1})\n",
    "        sess.run(train_data[2],feed_dict={users[2].x_in:batch_x3,users[2].y_true:batch_y3})\n",
    "        sess.run(train_data[4],feed_dict={users[4].x_in:batch_x5,users[4].y_true:batch_y5})\n",
    "        sess.run(train_data[6],feed_dict={users[6].x_in:batch_x7,users[6].y_true:batch_y7})\n",
    "        sess.run(train_data[8],feed_dict={users[8].x_in:batch_x9,users[8].y_true:batch_y9})\n",
    "\n",
    "        sess.run(train_data[1],feed_dict={users[1].x_in:batch_x2,users[1].y_true:batch_y2})\n",
    "        sess.run(train_data[3],feed_dict={users[3].x_in:batch_x4,users[3].y_true:batch_y4})\n",
    "        sess.run(train_data[5],feed_dict={users[5].x_in:batch_x6,users[5].y_true:batch_y6})\n",
    "        sess.run(train_data[7],feed_dict={users[7].x_in:batch_x8,users[7].y_true:batch_y8})\n",
    "        sess.run(train_data[9],feed_dict={users[9].x_in:batch_x10,users[9].y_true:batch_y10})\n",
    "        \n",
    "\n",
    "        lambda_w_update = []\n",
    "        lambda_b_update = []\n",
    "        \n",
    "        for i in range(numberOfUsers):\n",
    "            if i== 9:\n",
    "                break\n",
    "            user_a = users[i]\n",
    "            user_b = users[i+1]\n",
    "            lwu = lambdas_w_update(user_a,user_b)\n",
    "            lambda_w_update.append(lwu)\n",
    "            lbu = lambdas_b_update(user_a,user_b)\n",
    "            lambda_b_update.append(lbu)\n",
    "            \n",
    "\n",
    "        lambda_w_update\n",
    "        lambda_b_update\n",
    "        \n",
    "        matches1 = tf.equal(tf.argmax(users[0].y_pred,1),tf.argmax(users[0].y_true,1))\n",
    "        acc1 = tf.reduce_mean(tf.cast(matches1,tf.float32)) \n",
    "        matches2 = tf.equal(tf.argmax(users[1].y_pred,1),tf.argmax(users[1].y_true,1))\n",
    "        acc2 = tf.reduce_mean(tf.cast(matches2,tf.float32))\n",
    "        matches3 = tf.equal(tf.argmax(users[2].y_pred,1),tf.argmax(users[2].y_true,1))\n",
    "        acc3 = tf.reduce_mean(tf.cast(matches3,tf.float32))\n",
    "        matches4 = tf.equal(tf.argmax(users[3].y_pred,1),tf.argmax(users[3].y_true,1))\n",
    "        acc4 = tf.reduce_mean(tf.cast(matches4,tf.float32))\n",
    "        matches5 = tf.equal(tf.argmax(users[4].y_pred,1),tf.argmax(users[4].y_true,1))\n",
    "        acc5 = tf.reduce_mean(tf.cast(matches5,tf.float32))\n",
    "        matches6 = tf.equal(tf.argmax(users[5].y_pred,1),tf.argmax(users[5].y_true,1))\n",
    "        acc6 = tf.reduce_mean(tf.cast(matches6,tf.float32))\n",
    "        matches7 = tf.equal(tf.argmax(users[6].y_pred,1),tf.argmax(users[6].y_true,1))\n",
    "        acc7 = tf.reduce_mean(tf.cast(matches7,tf.float32))\n",
    "        matches8 = tf.equal(tf.argmax(users[7].y_pred,1),tf.argmax(users[7].y_true,1))\n",
    "        acc8 = tf.reduce_mean(tf.cast(matches8,tf.float32))\n",
    "        matches9 = tf.equal(tf.argmax(users[8].y_pred,1),tf.argmax(users[8].y_true,1))\n",
    "        acc9 = tf.reduce_mean(tf.cast(matches9,tf.float32))\n",
    "        matches10 = tf.equal(tf.argmax(users[9].y_pred,1),tf.argmax(users[9].y_true,1))\n",
    "        acc10 = tf.reduce_mean(tf.cast(matches10,tf.float32))\n",
    "        \n",
    "        \n",
    "        TestAcc1 = sess.run(acc1 ,feed_dict={users[0].x_in:x_test,users[0].y_true:y_test})\n",
    "        TestAcc2 = sess.run(acc2 ,feed_dict={users[1].x_in:x_test,users[1].y_true:y_test})\n",
    "        TestAcc3 = sess.run(acc3 ,feed_dict={users[2].x_in:x_test,users[2].y_true:y_test})     \n",
    "        TestAcc4 = sess.run(acc4 ,feed_dict={users[3].x_in:x_test,users[3].y_true:y_test})\n",
    "        TestAcc5 = sess.run(acc5 ,feed_dict={users[4].x_in:x_test,users[4].y_true:y_test})\n",
    "        TestAcc6 = sess.run(acc6 ,feed_dict={users[5].x_in:x_test,users[5].y_true:y_test})\n",
    "        TestAcc7 = sess.run(acc7 ,feed_dict={users[6].x_in:x_test,users[6].y_true:y_test})\n",
    "        TestAcc8 = sess.run(acc8 ,feed_dict={users[7].x_in:x_test,users[7].y_true:y_test})\n",
    "        TestAcc9 = sess.run(acc9 ,feed_dict={users[8].x_in:x_test,users[8].y_true:y_test})\n",
    "        TestAcc10 = sess.run(acc10 ,feed_dict={users[9].x_in:x_test,users[9].y_true:y_test})\n",
    "        \n",
    "        avg = (TestAcc1+TestAcc2+TestAcc3+TestAcc4+TestAcc5+TestAcc6+TestAcc7+TestAcc8+TestAcc9+TestAcc10)/10\n",
    "        avgAcc.append(avg)\n",
    "        \n",
    "        regW =0\n",
    "        regb =0\n",
    "        lambda_w = 0\n",
    "        lambda_b = 0\n",
    "\n",
    "        for i in range(numberOfUsers):\n",
    "            if i== 9:\n",
    "                break\n",
    "            user_a = users[i]\n",
    "            user_b = users[i+1]    \n",
    "            regW += rho/2*tf.nn.l2_loss(user_a.W1-user_b.W1)+ rho/2*tf.nn.l2_loss(user_a.W2-user_b.W2)+ rho/2*tf.nn.l2_loss(user_a.W3-user_b.W3)+ rho/2*tf.nn.l2_loss(user_a.W4-user_b.W4)+ rho/2*tf.nn.l2_loss(user_a.W5-user_b.W5)\n",
    "            regb += rho/2*tf.nn.l2_loss(user_a.b1-user_b.b1)+ rho/2*tf.nn.l2_loss(user_a.b2-user_b.b2)+ rho/2*tf.nn.l2_loss(user_a.b3-user_b.b3)+ rho/2*tf.nn.l2_loss(user_a.b4-user_b.b4)+ rho/2*tf.nn.l2_loss(user_a.b5-user_b.b5)\n",
    "            lambda_w += sum_lambdas_w(user_a, user_b)\n",
    "            lambda_b += sum_lambdas_b(user_a, user_b)\n",
    "\n",
    "        reg = regW+regb\n",
    "        lambdas = lambda_w + lambda_b\n",
    "        loss = cross_entropy + reg + lambdas\n",
    "        train_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
